{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "* Need to work out pagination in queries, and how to recognize current data length such that only query new records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install flipside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flipside import Flipside\n",
    "from config import flipside_api_key, flipside_nft_holder_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdk = Flipside(flipside_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title = \"_may_12\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Save Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/app/data/source\"\n",
    "\n",
    "def get_cwd():\n",
    "    cwd_temp = os.getcwd()\n",
    "    temp_split = cwd_temp.split('/')\n",
    "    cwd = \"\"\n",
    "    for x in temp_split:\n",
    "        if x == 'experiments':\n",
    "            break\n",
    "        elif x == '':\n",
    "            pass\n",
    "        else:\n",
    "            cwd += \"/\"+x       \n",
    "    return cwd\n",
    "\n",
    "\n",
    "def print_metrics(query):\n",
    "    started_at = query.run_stats.started_at\n",
    "    ended_at = query.run_stats.ended_at\n",
    "    elapsed_seconds = query.run_stats.elapsed_seconds\n",
    "    record_count = query.run_stats.record_count\n",
    "    print(f\"This query took ${elapsed_seconds} seconds to run and returned {record_count} records from the database.\")\n",
    "\n",
    "def query_and_save(_query, _filename, _page_size = 30000):\n",
    "    # Initial Query\n",
    "    print(f\"___\\n{_filename}\")\n",
    "    print(f\"querying page: 1\")\n",
    "    query_result_set = sdk.query(_query, page_size = _page_size)\n",
    "    df_output = pd.json_normalize(query_result_set.records)\n",
    "  \n",
    "    # Metrics\n",
    "    print_metrics(query_result_set)\n",
    "    \n",
    "    # Handle Pagination\n",
    "    if len(query_result_set.records) >= _page_size:\n",
    "        i = 2\n",
    "        keep_going = True\n",
    "        while keep_going:\n",
    "            print(f\"querying page: {i}\")\n",
    "            extended_result_set = sdk.get_query_results(\n",
    "                query_result_set.query_id,\n",
    "                page_number=i,\n",
    "                page_size=_page_size\n",
    "            )\n",
    "            # Metrics\n",
    "            print_metrics(query_result_set)\n",
    "            \n",
    "            # Concat Dataframes\n",
    "            df_local = pd.json_normalize(extended_result_set.records)\n",
    "            df_output = pd.concat([df_output, df_local]).reset_index()\n",
    "            \n",
    "            # Check if continue\n",
    "            print(len(extended_result_set.records) < _page_size)\n",
    "            print(len(extended_result_set.records))\n",
    "            if len(extended_result_set.records) < _page_size:\n",
    "                keep_going = False\n",
    "            i += 1\n",
    "    # Save\n",
    "    cwd = get_cwd()\n",
    "    full_filename = cwd+ data_path + '/' + filename+'.csv'\n",
    "    df_output.to_csv(full_filename) \n",
    "    \n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve Locker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_locker_address = '0x5f3b5DfEb7B28CDbD7FAba78963EE202a494e2A2'\n",
    "\n",
    "curve_locker_query = f\"\"\"SELECT \n",
    "  *,\n",
    "  WEEK(BLOCK_TIMESTAMP) as WEEK_NUMBER,\n",
    "  DAYOFWEEK(BLOCK_TIMESTAMP) as WEEK_DAY\n",
    "\n",
    "FROM ethereum.core.ez_decoded_event_logs\n",
    "WHERE CONTRACT_ADDRESS = lower('{curve_locker_address}')\n",
    "\"\"\"\n",
    "# AND BLOCK_TIMESTAMP <'2022-01-01 00:00:00.000'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'curve_locker'\n",
    "df_curve_locker = query_and_save(curve_locker_query, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve Gauge Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_voter_address = '0x2F50D538606Fa9EDD2B11E2446BEb18C9D5846bB'\n",
    "\n",
    "curve_gauge_vote_query = f\"\"\"SELECT \n",
    "    SYMBOL, \n",
    "    NAME, \n",
    "    WEEK(BLOCK_TIMESTAMP) as WEEK_NUMBER,\n",
    "    DAYOFWEEK(BLOCK_TIMESTAMP) as WEEK_DAY,\n",
    "    DECODED_LOG,\n",
    "    TX_HASH,\n",
    "    BLOCK_TIMESTAMP\n",
    "\n",
    "FROM ethereum.core.ez_decoded_event_logs LOGS\n",
    " LEFT JOIN ethereum.core.dim_contracts CONTRACT\n",
    "   ON CONTRACT.address = lower(LOGS.DECODED_LOG:gauge_addr::string)\n",
    "WHERE CONTRACT_ADDRESS = lower('{curve_voter_address}')\n",
    "AND EVENT_NAME = 'VoteForGauge'\n",
    "ORDER BY BLOCK_TIMESTAMP ASC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'curve_gauge_votes'\n",
    "df_curve_locker = query_and_save(curve_gauge_vote_query, filename, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshot Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convex_snapshot_votes_query = f\"\"\"\n",
    "SELECT \n",
    "    PROPOSAL_ID, \n",
    "    PROPOSAL_START_TIME, \n",
    "    PROPOSAL_END_TIME, \n",
    "    PROPOSAL_TITLE, \n",
    "    PROPOSAL_AUTHOR,\n",
    "    VOTE_OPTION,\n",
    "    VOTING_POWER,\n",
    "    VOTE_TIMESTAMP,\n",
    "    QUORUM,\n",
    "    CHOICES,\n",
    "    VOTING_PERIOD,\n",
    "    NETWORK,\n",
    "    SPACE_ID,\n",
    "    VOTER,\n",
    "    ADDRESS_NAME,\n",
    "    LABEL_TYPE, \n",
    "    LABEL_SUBTYPE,\n",
    "    LABEL\n",
    "FROM ethereum.core.ez_snapshot SNAPSHOT\n",
    " LEFT JOIN ethereum.core.dim_labels LABELS\n",
    "   ON SNAPSHOT.VOTER = LABELS.ADDRESS\n",
    "WHERE SPACE_ID = 'cvx.eth' \n",
    "AND PROPOSAL_TITLE LIKE 'Gauge Weight%'\n",
    "AND VOTE_TIMESTAMP > '2022-12-20 00:00:00.000'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'convex_snapshot_votes'\n",
    "df_snapshot_votes = query_and_save(convex_snapshot_votes_query, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference: Gauge to LP map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_voter_address = '0x2F50D538606Fa9EDD2B11E2446BEb18C9D5846bB'\n",
    "\n",
    "gauge_to_lp_map_query = \"\"\"\n",
    "with v2_deployer as (\n",
    "  SELECT\n",
    "    BLOCK_TIMESTAMP as block_timestamp,\n",
    "    DECODED_LOG:gauge::string as gauge_addr,\n",
    "    DECODED_LOG:pool::string as pool_addr,\n",
    "    DECODED_LOG:token::string as token_addr,\n",
    "    'v2' as type\n",
    "  \n",
    "  \n",
    "  FROM ethereum.core.ez_decoded_event_logs\n",
    "  WHERE CONTRACT_ADDRESS = lower('0xF18056Bbd320E96A48e3Fbf8bC061322531aac99') -- v2 deployer\n",
    "  AND EVENT_NAME = 'LiquidityGaugeDeployed'\n",
    "\n",
    "),\n",
    "\n",
    "factory_deployer as (\n",
    "  SELECT \n",
    "    BLOCK_TIMESTAMP as block_timestamp,\n",
    "    DECODED_LOG:gauge::string as gauge_addr,\n",
    "    DECODED_LOG:pool::string as pool_addr,\n",
    "    '' as token_addr,\n",
    "    'factory' as type\n",
    "\n",
    "  FROM ethereum.core.ez_decoded_event_logs as logs\n",
    "\n",
    "  WHERE logs.CONTRACT_ADDRESS = lower('0xB9fC157394Af804a3578134A6585C0dc9cc990d4')  -- factory\n",
    "  AND logs.EVENT_NAME = 'LiquidityGaugeDeployed'\n",
    "),\n",
    "\n",
    "\n",
    "combo as (\n",
    "  SELECT gauge_addr, pool_addr, token_addr, type, block_timestamp FROM v2_deployer\n",
    "  UNION\n",
    "  SELECT gauge_addr, pool_addr, token_addr, type, block_timestamp  FROM factory_deployer\n",
    ")\n",
    "\n",
    "SELECT \n",
    "  combo.gauge_addr as GAUGE_ADDR, \n",
    "  combo.pool_addr as POOL_ADDR, \n",
    "  combo.token_addr as TOKEN_ADDR, \n",
    "  combo.type as TYPE, \n",
    "  combo.block_timestamp as BLOCK_TIMESTAMP,\n",
    "  contracts.NAME as GAUGE_NAME,\n",
    "  contracts.SYMBOL as GAUGE_SYMBOL,\n",
    "  pool_contracts.NAME as POOL_NAME,\n",
    "  pool_contracts.SYMBOL as POOL_SYMBOL,\n",
    "  token_contracts.NAME as TOKEN_NAME,\n",
    "  token_contracts.SYMBOL as TOKEN_SYMBOL\n",
    "FROM combo\n",
    "LEFT JOIN ethereum.core.dim_contracts as contracts\n",
    "  ON combo.gauge_addr = contracts.ADDRESS\n",
    "LEFT JOIN ethereum.core.dim_contracts as pool_contracts\n",
    "  ON combo.pool_addr = pool_contracts.ADDRESS\n",
    "LEFT JOIN ethereum.core.dim_contracts as token_contracts\n",
    "  ON combo.token_addr = token_contracts.ADDRESS\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gauge_to_lp_map'\n",
    "df_curve_locker = query_and_save(gauge_to_lp_map_query, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
