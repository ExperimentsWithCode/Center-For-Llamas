{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flipside import Flipside\n",
    "from config import flipside_api_key, flipside_nft_holder_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.data.local_storage import (\n",
    "    read_json,\n",
    "    read_csv\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdk = Flipside(flipside_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "* [X] Curve Locks\n",
    "* [X] Curve Votes\n",
    "\n",
    "* [X] Gauge to LP map\n",
    "* [] Curve Liquidity\n",
    "\n",
    "* [X] Convex Snapshot\n",
    "* [X] Votium Bounties\n",
    "\n",
    "* [X] StakeDAO Snapshot\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Save Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/app/data/source\"\n",
    "\n",
    "def get_cwd():\n",
    "    cwd_temp = os.getcwd()\n",
    "    temp_split = cwd_temp.split('/')\n",
    "    cwd = \"\"\n",
    "    for x in temp_split:\n",
    "        if x == 'experiments':\n",
    "            break\n",
    "        elif x == '':\n",
    "            pass\n",
    "        else:\n",
    "            cwd += \"/\"+x       \n",
    "    return cwd\n",
    "\n",
    "def print_metrics(query):\n",
    "    started_at = query.run_stats.started_at\n",
    "    ended_at = query.run_stats.ended_at\n",
    "    elapsed_seconds = query.run_stats.elapsed_seconds\n",
    "    record_count = query.run_stats.record_count\n",
    "    print(f\"This query took ${elapsed_seconds} seconds to run and returned {record_count} records from the database.\")\n",
    "\n",
    "\n",
    "def query_and_save(_query, _filename, _df_base = [], _page_size = 5000):\n",
    "    try:\n",
    "        # Initial Query\n",
    "        if len(_df_base) > 0:\n",
    "            print(\"based\")\n",
    "            df_output = _df_base\n",
    "        else:\n",
    "            df_output = []\n",
    "\n",
    "        print(f\"___\\n{_filename}\")\n",
    "        print(f\"querying page: 1\")\n",
    "        query_result_set = sdk.query(_query, page_size = _page_size)\n",
    "        if len(df_output) == 0:\n",
    "            df_output = pd.json_normalize(query_result_set.records)\n",
    "        else:\n",
    "            # Concat Dataframes\n",
    "            df_local = pd.json_normalize(query_result_set.records)\n",
    "            df_output = pd.concat([df_output, df_local], ignore_index=True)\n",
    "\n",
    "        # Metrics\n",
    "        print_metrics(query_result_set)\n",
    "        i = 2\n",
    "            \n",
    "        # Handle Pagination\n",
    "        if len(query_result_set.records) >= _page_size:\n",
    "            keep_going = True\n",
    "            while keep_going:\n",
    "                print(f\"querying page: {i}\")\n",
    "                extended_result_set = sdk.get_query_results(\n",
    "                    query_result_set.query_id,\n",
    "                    page_number=i,\n",
    "                    page_size=_page_size\n",
    "                )\n",
    "                # Metrics\n",
    "                print_metrics(query_result_set)\n",
    "                \n",
    "                # Concat Dataframes\n",
    "                df_local = pd.json_normalize(extended_result_set.records)\n",
    "                df_output = pd.concat([df_output, df_local], ignore_index=True)\n",
    "                \n",
    "                # Check if continue\n",
    "                print(len(extended_result_set.records) < _page_size)\n",
    "                print(len(extended_result_set.records))\n",
    "                if len(extended_result_set.records) < _page_size:\n",
    "                    keep_going = False\n",
    "                i += 1\n",
    "        # Save\n",
    "        cwd = get_cwd()\n",
    "        full_filename = cwd+ data_path + '/' + filename+'.csv'\n",
    "        df_output.to_csv(full_filename) \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        if len(_df_base) > 0:\n",
    "            df_output = _df_base\n",
    "        else:\n",
    "            df_output = []\n",
    "\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_and_target(filename, target = 'block_timestamp'):\n",
    "    # gets dataframe and max value of target intended for time values\n",
    "    resp_dict = read_csv(filename, 'source')\n",
    "    df = pd.json_normalize(resp_dict)\n",
    "    print(df.keys())\n",
    "\n",
    "    # Get Max Value of target\n",
    "    temp_df = df.sort_values(target).tail(1)\n",
    "    search_result = temp_df.iloc[0][target] \n",
    "    # # If date only drown out time diffs by removing last\n",
    "    # if target == 'date_day':\n",
    "    #     # remove most current\n",
    "    #     df = df[df[target] < search_result]\n",
    "    #     # find new max value of target\n",
    "    #     temp_df = df.sort_values(target).tail(1)\n",
    "    #     search_result = temp_df.iloc[0][target] \n",
    "\n",
    "    try:\n",
    "        if 'T' in search_result:\n",
    "            split = search_result.split(\"T\")\n",
    "            search_result = split[0]+\" \"+split[1][:-1]\n",
    "    except:\n",
    "        pass\n",
    "    print(search_result)\n",
    "    return df, search_result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve Locker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'curve_locker'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, block_timestamp = get_df_and_target(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_locker_address = '0x5f3b5DfEb7B28CDbD7FAba78963EE202a494e2A2'\n",
    "\n",
    "curve_locker_query = f\"\"\"SELECT \n",
    "  *,\n",
    "  WEEK(BLOCK_TIMESTAMP) as WEEK_NUMBER,\n",
    "  DAYOFWEEK(BLOCK_TIMESTAMP) as WEEK_DAY\n",
    "\n",
    "FROM ethereum.core.ez_decoded_event_logs\n",
    "WHERE CONTRACT_ADDRESS = lower('{curve_locker_address}')\n",
    "AND BLOCK_TIMESTAMP >'{block_timestamp}'\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_curve_locker = query_and_save(curve_locker_query, filename, df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve Gauge Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'curve_gauge_votes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, block_timstamp = get_df_and_target(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_voter_address = '0x2F50D538606Fa9EDD2B11E2446BEb18C9D5846bB'\n",
    "\n",
    "curve_gauge_vote_query = f\"\"\"SELECT \n",
    "    SYMBOL, \n",
    "    NAME, \n",
    "    WEEK(BLOCK_TIMESTAMP) as WEEK_NUMBER,\n",
    "    DAYOFWEEK(BLOCK_TIMESTAMP) as WEEK_DAY,\n",
    "    DECODED_LOG,\n",
    "    TX_HASH,\n",
    "    BLOCK_TIMESTAMP\n",
    "\n",
    "FROM ethereum.core.ez_decoded_event_logs LOGS\n",
    " LEFT JOIN ethereum.core.dim_contracts CONTRACT\n",
    "   ON CONTRACT.address = lower(LOGS.DECODED_LOG:gauge_addr::string)\n",
    "WHERE CONTRACT_ADDRESS = lower('{curve_voter_address}')\n",
    "AND EVENT_NAME = 'VoteForGauge'\n",
    "AND BLOCK_TIMESTAMP >'{block_timestamp}'\n",
    "ORDER BY BLOCK_TIMESTAMP ASC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_curve_guage_votes = query_and_save(curve_gauge_vote_query, filename, df, 5000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshot Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'convex_snapshot_votes'\n",
    "df, vote_timestamp = get_df_and_target(filename, 'vote_timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convex_snapshot_votes_query = f\"\"\"\n",
    "SELECT \n",
    "    PROPOSAL_ID, \n",
    "    PROPOSAL_START_TIME, \n",
    "    PROPOSAL_END_TIME, \n",
    "    PROPOSAL_TITLE, \n",
    "    PROPOSAL_AUTHOR,\n",
    "    VOTE_OPTION,\n",
    "    VOTING_POWER,\n",
    "    VOTE_TIMESTAMP,\n",
    "    QUORUM,\n",
    "    CHOICES,\n",
    "    VOTING_PERIOD,\n",
    "    NETWORK,\n",
    "    SPACE_ID,\n",
    "    VOTER,\n",
    "    ADDRESS_NAME,\n",
    "    LABEL_TYPE, \n",
    "    LABEL_SUBTYPE,\n",
    "    LABEL\n",
    "FROM external.snapshot.ez_snapshot as SNAPSHOT\n",
    " LEFT JOIN ethereum.core.dim_labels LABELS\n",
    "   ON SNAPSHOT.VOTER = LABELS.ADDRESS\n",
    "WHERE SPACE_ID = 'cvx.eth' \n",
    "AND PROPOSAL_TITLE LIKE 'Gauge Weight%'\n",
    "AND VOTE_TIMESTAMP > '{vote_timestamp}'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snapshot_votes = query_and_save(convex_snapshot_votes_query, filename, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'stakedao_snapshot'\n",
    "df, vote_timestamp = get_df_and_target(filename, 'vote_timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stakedao_snapshot_votes_query = f\"\"\"\n",
    "SELECT \n",
    "    PROPOSAL_ID, \n",
    "    PROPOSAL_START_TIME, \n",
    "    PROPOSAL_END_TIME, \n",
    "    PROPOSAL_TITLE, \n",
    "    PROPOSAL_AUTHOR,\n",
    "    VOTE_OPTION,\n",
    "    VOTING_POWER,\n",
    "    VOTE_TIMESTAMP,\n",
    "    QUORUM,\n",
    "    CHOICES,\n",
    "    VOTING_PERIOD,\n",
    "    NETWORK,\n",
    "    SPACE_ID,\n",
    "    VOTER,\n",
    "    ADDRESS_NAME,\n",
    "    LABEL_TYPE, \n",
    "    LABEL_SUBTYPE,\n",
    "    LABEL\n",
    "FROM external.snapshot.ez_snapshot as SNAPSHOT\n",
    " LEFT JOIN ethereum.core.dim_labels LABELS\n",
    "   ON SNAPSHOT.VOTER = LABELS.ADDRESS\n",
    "WHERE SPACE_ID = 'sdcrv.eth' \n",
    "AND (PROPOSAL_TITLE LIKE 'Gauge vote - CRV%'\n",
    "OR\n",
    "PROPOSAL_TITLE LIKE 'Gauge vote CRV%'\n",
    ")\n",
    "AND VOTE_TIMESTAMP > '{vote_timestamp}'\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snapshot_votes = query_and_save(stakedao_snapshot_votes_query, filename, df,  1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snapshot_votes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Votium Bounties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'votium_bounty_for_round'\n",
    "df, block_timestamp = get_df_and_target(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votium_bounty_query = f\"\"\"\n",
    "SELECT \n",
    "  EVENT_NAME,\n",
    "  TX_HASH,\n",
    "  DECODED_LOG:_choiceIndex::int as choice_index,\n",
    "  DECODED_LOG:_amount::int / pow(10,18) as amount,\n",
    "  DECODED_LOG:_proposal::string as proposal_id,\n",
    "  DECODED_LOG:_token::string as bounty_token_address,\n",
    "  ORIGIN_FROM_ADDRESS,\n",
    "  BLOCK_TIMESTAMP as block_timestamp,\n",
    "  PRICE as price,\n",
    "  price * amount as bounty_value,\n",
    "  contracts.NAME as token_name,\n",
    "  contracts.SYMBOL as token_symbol\n",
    "FROM ethereum.core.ez_decoded_event_logs as logs\n",
    "\n",
    "LEFT JOIN ethereum.core.fact_hourly_token_prices as prices\n",
    "ON logs.DECODED_LOG:_token = prices.token_address\n",
    "AND time_slice(logs.BLOCK_TIMESTAMP::timestamp_ntz, 1, 'HOUR') = prices.HOUR\n",
    "\n",
    "LEFT JOIN ethereum.core.dim_contracts as contracts\n",
    "ON logs.DECODED_LOG:_token = contracts.ADDRESS\n",
    "\n",
    "WHERE CONTRACT_ADDRESS= lower('0x19bbc3463dd8d07f55438014b021fb457ebd4595')\n",
    "\n",
    "AND EVENT_NAME = 'Bribed'\n",
    "AND BLOCK_TIMESTAMP > '{block_timestamp}'\n",
    "\n",
    "\n",
    "ORDER BY block_timestamp\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_votium_bounty = query_and_save(votium_bounty_query, filename, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference: Gauge to LP Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gauge_to_lp_map'\n",
    "df, block_timestamp = get_df_and_target(filename, 'deployed_timestamp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_voter_address = '0x2F50D538606Fa9EDD2B11E2446BEb18C9D5846bB'\n",
    "\n",
    "gauge_to_lp_map_query = f\"\"\"\n",
    "-- 0x2F50D538606Fa9EDD2B11E2446BEb18C9D5846bB\n",
    "with gauge_types as (\n",
    "  SELECT \n",
    "    DECODED_LOG:name::string as name,\n",
    "    DECODED_LOG:type_id::int as type_id,\n",
    "    BLOCK_TIMESTAMP as block_timestamp,\n",
    "    TX_HASH as tx_hash\n",
    "  \n",
    "  FROM ethereum.core.ez_decoded_event_logs LOGS\n",
    "   LEFT JOIN ethereum.core.dim_contracts CONTRACT\n",
    "     ON CONTRACT.address = lower(LOGS.DECODED_LOG:gauge_addr::string)\n",
    "  WHERE CONTRACT_ADDRESS = lower('{curve_voter_address}')\n",
    "  AND EVENT_NAME = 'AddType'\n",
    "),\n",
    "\n",
    "new_type_weight as (\n",
    "  SELECT \n",
    "    ifnull(DECODED_LOG:time::int, 0) as time,\n",
    "    DECODED_LOG:total_weight::string as total_weight,\n",
    "    ifnull(DECODED_LOG:type_id::int, 0) as type_id,\n",
    "    DECODED_LOG:weight::string as weight,\n",
    "    TX_HASH,\n",
    "    BLOCK_TIMESTAMP\n",
    "  \n",
    "  FROM ethereum.core.ez_decoded_event_logs\n",
    "  WHERE CONTRACT_ADDRESS = lower('{curve_voter_address}')\n",
    "  AND EVENT_NAME = 'NewTypeWeight'\n",
    "  ORDER BY BLOCK_TIMESTAMP ASC\n",
    "\n",
    "),\n",
    "\n",
    "new_gauges as (\n",
    "\n",
    "  SELECT \n",
    "      SYMBOL, \n",
    "      NAME, \n",
    "      DECODED_LOG:addr::string as gauge_addr,\n",
    "      DECODED_LOG:gauge_type::int as gauge_type,\n",
    "      DECODED_LOG:weight::int as weight,\n",
    "      TX_HASH,\n",
    "      BLOCK_TIMESTAMP\n",
    "  \n",
    "  FROM ethereum.core.ez_decoded_event_logs LOGS\n",
    "   LEFT JOIN ethereum.core.dim_contracts CONTRACT\n",
    "     ON CONTRACT.address = lower(LOGS.DECODED_LOG:addr::string)\n",
    "  WHERE CONTRACT_ADDRESS = lower('{curve_voter_address}')\n",
    "  AND EVENT_NAME = 'NewGauge'\n",
    "  ORDER BY BLOCK_TIMESTAMP ASC\n",
    "  \n",
    "),\n",
    "\n",
    "gauge_meta as (\n",
    "  SELECT \n",
    "    new_gauges.gauge_type as type_id,\n",
    "    gauge_types.name as type_name,\n",
    "    new_gauges.name,\n",
    "    new_gauges.symbol,\n",
    "    new_gauges.gauge_addr,\n",
    "    new_gauges.weight,\n",
    "    new_type_weight.weight as type_weight,\n",
    "    new_type_weight.total_weight as type_total_weight,\n",
    "    new_type_weight.time as type_weight_time,\n",
    "    new_gauges.tx_hash,\n",
    "    new_gauges.block_timestamp as vote_timestamp\n",
    "  FROM new_gauges \n",
    "  LEFT JOIN gauge_types\n",
    "  ON new_gauges.gauge_type = gauge_types.type_id\n",
    "  LEFT JOIN new_type_weight\n",
    "  ON new_gauges.gauge_type = new_type_weight.type_id\n",
    "  ORDER BY new_gauges.block_timestamp DESC\n",
    "),\n",
    "\n",
    "\n",
    "-- BREAK between meta and deployers\n",
    "\n",
    "v2_deployer as (\n",
    "  SELECT\n",
    "    BLOCK_TIMESTAMP as block_timestamp,\n",
    "    DECODED_LOG:gauge::string as gauge_addr,\n",
    "    DECODED_LOG:pool::string as pool_addr,\n",
    "    DECODED_LOG:token::string as token_addr,\n",
    "    'v2' as source\n",
    "      \n",
    "  FROM ethereum.core.ez_decoded_event_logs\n",
    "  WHERE CONTRACT_ADDRESS = lower('0xF18056Bbd320E96A48e3Fbf8bC061322531aac99') -- v2 deployer\n",
    "  AND EVENT_NAME = 'LiquidityGaugeDeployed'\n",
    "  AND BLOCK_TIMESTAMP > '{block_timestamp}'\n",
    "\n",
    "\n",
    "),\n",
    "\n",
    "factory_deployer as (\n",
    "  SELECT \n",
    "    BLOCK_TIMESTAMP as block_timestamp,\n",
    "    DECODED_LOG:gauge::string as gauge_addr,\n",
    "    DECODED_LOG:pool::string as pool_addr,\n",
    "    '' as token_addr,\n",
    "    'factory' as source\n",
    "\n",
    "  FROM ethereum.core.ez_decoded_event_logs as logs\n",
    "\n",
    "  WHERE logs.CONTRACT_ADDRESS = lower('0xB9fC157394Af804a3578134A6585C0dc9cc990d4')  -- factory\n",
    "  AND logs.EVENT_NAME = 'LiquidityGaugeDeployed'\n",
    "  AND BLOCK_TIMESTAMP > '{block_timestamp}'\n",
    "\n",
    "),\n",
    "\n",
    "\n",
    "stable_deployer as (\n",
    "  SELECT \n",
    "    BLOCK_TIMESTAMP as block_timestamp,\n",
    "    DECODED_LOG:gauge::string as gauge_addr,\n",
    "    DECODED_LOG:pool::string as pool_addr,\n",
    "    '' as token_addr,\n",
    "    'stable_factory' as source\n",
    "\n",
    "  FROM ethereum.core.ez_decoded_event_logs as logs\n",
    "\n",
    "  WHERE logs.CONTRACT_ADDRESS = lower('0x4F8846Ae9380B90d2E71D5e3D042dff3E7ebb40d')  -- stable_factory\n",
    "  AND logs.EVENT_NAME = 'LiquidityGaugeDeployed'\n",
    "  AND BLOCK_TIMESTAMP > '{block_timestamp}'\n",
    "\n",
    "),\n",
    "\n",
    "\n",
    "multichain_deployer as (\n",
    "  SELECT \n",
    "    BLOCK_TIMESTAMP as block_timestamp,\n",
    "    DECODED_LOG:_gauge::string as gauge_addr,\n",
    "    DECODED_LOG:_chain_id::string as chain_id,\n",
    "    DECODED_LOG:_deployer::string as deployer,\n",
    "    DECODED_LOG:_implementation::string as implementation,\n",
    "    DECODED_LOG:_salt::string as salt,\n",
    "    '' as pool_addr,\n",
    "    '' as token_addr,\n",
    "    'multichain_factory' as source\n",
    "\n",
    "  FROM ethereum.core.ez_decoded_event_logs as logs\n",
    "\n",
    "  WHERE logs.CONTRACT_ADDRESS = lower('0xabc000d88f23bb45525e447528dbf656a9d55bf5')  -- multichain_factory\n",
    "  AND logs.EVENT_NAME = 'DeployedGauge'\n",
    "  AND BLOCK_TIMESTAMP > '{block_timestamp}'\n",
    "\n",
    "),\n",
    "\n",
    "tricrypto_deployer as (\n",
    "  SELECT \n",
    "    BLOCK_TIMESTAMP as block_timestamp,\n",
    "    DECODED_LOG:gauge::string as gauge_addr,\n",
    "    DECODED_LOG:pool::string as pool_addr,\n",
    "    '' as token_addr,\n",
    "    'tricrypto_factory' as source\n",
    "\n",
    "  FROM ethereum.core.ez_decoded_event_logs as logs\n",
    "\n",
    "  WHERE logs.CONTRACT_ADDRESS = lower('0x0c0e5f2ff0ff18a3be9b835635039256dc4b4963')  -- tricrypto_factory\n",
    "  AND logs.EVENT_NAME = 'LiquidityGaugeDeployed'\n",
    "  AND BLOCK_TIMESTAMP > '{block_timestamp}'\n",
    "),\n",
    "\n",
    "\n",
    "combo as (\n",
    "  SELECT gauge_addr, pool_addr, token_addr, source, block_timestamp, '' as chain_id FROM v2_deployer\n",
    "  UNION\n",
    "  SELECT gauge_addr, pool_addr, token_addr, source, block_timestamp,'' as chain_id   FROM factory_deployer\n",
    "  UNION\n",
    "  SELECT gauge_addr, pool_addr, token_addr, source, block_timestamp,'' as chain_id FROM stable_deployer\n",
    "  UNION\n",
    "  SELECT gauge_addr, pool_addr, token_addr, source, block_timestamp,'' as chain_id FROM tricrypto_deployer\n",
    "  UNION\n",
    "  SELECT gauge_addr, pool_addr, token_addr, source, block_timestamp, chain_id  FROM multichain_deployer\n",
    "\n",
    "),\n",
    "\n",
    "deployer_meta as (\n",
    "  SELECT \n",
    "    combo.gauge_addr as gauge_addr, \n",
    "    combo.pool_addr as pool_addr, \n",
    "    combo.token_addr as token_addr, \n",
    "    combo.source as source, \n",
    "    combo.chain_id as chain_id,\n",
    "    combo.block_timestamp as block_timestamp,\n",
    "    contracts.NAME as gauge_name,\n",
    "    contracts.SYMBOL as gauge_symbol,\n",
    "    pool_contracts.NAME as pool_name,\n",
    "    pool_contracts.SYMBOL as pool_symbol,\n",
    "    token_contracts.NAME as token_name,\n",
    "    token_contracts.SYMBOL as token_symbol\n",
    "  FROM combo\n",
    "  LEFT JOIN ethereum.core.dim_contracts as contracts\n",
    "    ON combo.gauge_addr = contracts.ADDRESS\n",
    "  LEFT JOIN ethereum.core.dim_contracts as pool_contracts\n",
    "    ON combo.pool_addr = pool_contracts.ADDRESS\n",
    "  LEFT JOIN ethereum.core.dim_contracts as token_contracts\n",
    "    ON combo.token_addr = token_contracts.ADDRESS\n",
    ")\n",
    "\n",
    "\n",
    "SELECT \n",
    "    gauge_meta.type_id,\n",
    "    gauge_meta.type_name,\n",
    "    IFNULL(gauge_meta.name, deployer_meta.gauge_name) as name,\n",
    "    IFNULL(gauge_meta.symbol, deployer_meta.gauge_symbol) as symbol,\n",
    "    IFNULL(gauge_meta.gauge_addr, deployer_meta.gauge_addr) as gauge_addr,\n",
    "    gauge_meta.weight,\n",
    "    gauge_meta.type_weight,\n",
    "    gauge_meta.type_total_weight,\n",
    "    gauge_meta.type_weight_time,\n",
    "    gauge_meta.tx_hash,\n",
    "    gauge_meta.vote_timestamp,\n",
    "\n",
    "    deployer_meta.pool_addr, \n",
    "    deployer_meta.token_addr, \n",
    "    deployer_meta.source, \n",
    "    deployer_meta.block_timestamp as deployed_timestamp,\n",
    "    deployer_meta.gauge_name,\n",
    "    deployer_meta.gauge_symbol,\n",
    "    deployer_meta.pool_name,\n",
    "    deployer_meta.pool_symbol,\n",
    "    deployer_meta.token_name,\n",
    "    deployer_meta.token_symbol,\n",
    "    deployer_meta.chain_id\n",
    "FROM gauge_meta\n",
    "FULL JOIN deployer_meta\n",
    "ON gauge_meta.gauge_addr = deployer_meta.gauge_addr\n",
    "ORDER BY deployed_timestamp DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_curve_gauge_map = query_and_save(gauge_to_lp_map_query, filename, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curve Liquidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'liquidity_general'\n",
    "df, date_day = get_df_and_target(filename, 'date_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_day.date() + timedelta(days=1)\n",
    "# tomorow_date = dt.strptime(date_day,'%Y-%m-%d %H:%M:%S.%f') + timedelta(days=1)\n",
    "\n",
    "# print(date_day)\n",
    "# print(tomorow_date)\n",
    "\n",
    "df = df[df['date_day'] < date_day]\n",
    "df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2023-03-01 00:00:00.000'\n",
    "curve_liquidity_query = f\"\"\"\n",
    "with curve_swaps as (\n",
    "SELECT\n",
    "*\n",
    "FROM ethereum.core.ez_dex_swaps\n",
    "WHERE PLATFORM = 'curve' \n",
    "),\n",
    "\n",
    "tokens_out as (\n",
    "SELECT \n",
    "DISTINCT TOKEN_OUT as token,\n",
    "CONTRACT_ADDRESS as pool_address,\n",
    "POOL_NAME as pool_name,\n",
    "SYMBOL_OUT as symbol\n",
    "FROM curve_swaps\n",
    "GROUP BY TOKEN_OUT, SYMBOL_OUT, CONTRACT_ADDRESS, POOL_NAME\n",
    "),\n",
    "\n",
    "tokens_in as (\n",
    "SELECT \n",
    "DISTINCT TOKEN_IN as token,\n",
    "CONTRACT_ADDRESS as pool_address,\n",
    "POOL_NAME as pool_name,\n",
    "\n",
    "SYMBOL_IN  as symbol\n",
    "FROM curve_swaps\n",
    "-- GROUP BY TOKEN_IN\n",
    "),\n",
    "\n",
    "pools as (\n",
    "SELECT\n",
    "DISTINCT CONTRACT_ADDRESS as pool_address,\n",
    "POOL_NAME as pool_name,\n",
    "'ETH' as symbol,\n",
    "'' as token\n",
    "FROM curve_swaps\n",
    "-- GROUP BY CONTRACT_ADDRESS\n",
    "\n",
    "),\n",
    "\n",
    "tokens as (\n",
    "select \n",
    "*\n",
    "FROM tokens_out\n",
    "UNION all\n",
    "select \n",
    "*\n",
    "FROM tokens_in\n",
    "UNION all\n",
    "select \n",
    "token, pool_address, pool_name, symbol\n",
    "FROM pools\n",
    "),\n",
    "\n",
    "all_tokens as (\n",
    "select \n",
    "* \n",
    "from tokens\n",
    "group by token, symbol, pool_address, pool_name\n",
    "),\n",
    "\n",
    "\n",
    "-- COMBINE TOKENS WITH DATES\n",
    "dates as (\n",
    "select \n",
    "date_day \n",
    "from ethereum.core.dim_dates \n",
    "where date_day between '{date_day}' and current_date()\n",
    "),\n",
    "\n",
    "dates_x_tokens as (\n",
    "select \n",
    "date_day,\n",
    "symbol,\n",
    "token,\n",
    "pool_address,\n",
    "pool_name\n",
    "\n",
    "from dates\n",
    "cross join all_tokens\n",
    "ORDER BY date_day DESC\n",
    "),\n",
    "\n",
    "\n",
    "dates_x_tokens_x_price as (\n",
    "select \n",
    "dates_x_tokens.date_day,\n",
    "dates_x_tokens.symbol,\n",
    "dates_x_tokens.token,\n",
    "pool_address,\n",
    "pool_name,\n",
    "AVG(prices.PRICE) as daily_price\n",
    "FROM dates_x_tokens\n",
    "LEFT JOIN ethereum.core.fact_hourly_token_prices as prices\n",
    "ON (dates_x_tokens.token = prices.TOKEN_ADDRESS \n",
    "  OR dates_x_tokens.token = prices.SYMBOL\n",
    "  )\n",
    "AND dates_x_tokens.date_day = prices.hour::date\n",
    "GROUP BY (\n",
    "  dates_x_tokens.date_day,\n",
    "  dates_x_tokens.symbol,\n",
    "  dates_x_tokens.token,\n",
    "  dates_x_tokens.pool_address,\n",
    "  dates_x_tokens.pool_name\n",
    "  )\n",
    "ORDER BY date_day DESC\n",
    "\n",
    "),\n",
    "\n",
    "\n",
    "balances as (\n",
    "select\n",
    "\n",
    "dates.date_day,\n",
    "dates.pool_address,\n",
    "dates.pool_name, \n",
    "dates.token,\n",
    "dates.symbol,\n",
    "sum(deltas.BAL_DELTA) as balance,\n",
    "(balance * dates.daily_price) as balance_usd \n",
    "\n",
    "from dates_x_tokens_x_price as dates\n",
    "left join ethereum.core.ez_balance_deltas as deltas\n",
    "ON deltas.BLOCK_TIMESTAMP::date <= dates.date_day\n",
    "AND deltas.USER_ADDRESS = dates.pool_address\n",
    "AND (deltas.CONTRACT_ADDRESS = dates.token\n",
    "OR dates.symbol = deltas.SYMBOL)\n",
    "GROUP BY (\n",
    "  dates.date_day,\n",
    "  dates.pool_address,\n",
    "  dates.pool_name, \n",
    "  dates.token,\n",
    "  dates.symbol,\n",
    "  dates.daily_price\n",
    "  )\n",
    "ORDER BY date_day DESC\n",
    "\n",
    ") \n",
    "\n",
    "\n",
    "SELECT\n",
    "  date_day, \n",
    "  pool_name,\n",
    "  sum(balance) as current_bal,\n",
    "  sum(balance_usd) as current_bal_usd,\n",
    "  LISTAGG(symbol, ', ') as tradeable_assets,\n",
    "  pool_address\n",
    "FROM balances\n",
    "GROUP BY date_day, pool_address, pool_name\n",
    "ORDER BY date_day DESC\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_liquidity_general = query_and_save(curve_liquidity_query, filename, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
